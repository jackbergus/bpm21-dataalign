\section{Introduction}
\label{sec:introduction}

\textit{Conformance checking} is a branch of process mining  assessing whether a sequence of distinguishable events (i.e., a \textit{trace}) conforms to the expected process behavior represented as a \textit{process model} \cite{RozinatA08}. When a trace does not conform to the model, we say that the trace is \textit{deviant}. In this case, techniques based on cost-driven alignments additionally provide minimal repair strategies to make the trace conformant to the model \cite{LeoniA13}. Alignments represent a valuable instrument for business analysts, as the combined provision of alternative repair strategies to make a trace conformant ranked by alignment cost supports the business analyst in choosing among different process improvement strategies. In conformance checking, models can be described by either procedural or declarative languages: while the former fully enumerate the set of all the possible allowed traces, the latter provide a compact process representation by listing the constraints that delimit the expected behavior \cite{LeoniA13,Westergaard11}. Nevertheless, \DIFdel{LTL$_f$-based} declarative models expressed in \DIFadd{Linear Time Logic on Finite Traces (LTL$_f$)} can always be transformed into procedural via their associated constraint automata. \DIFadd{As LTL$_f$ is an extension of modal logic in which worlds are organized in an finite linear structure, this logic is well suited to describe business processes logs having traces of finite length \cite{GiacomoV13}.}

%In fact, such semantics describes the actions that will follow when some pre-conditions are met \cite{LiPZVR20}.
The representation of declarative process models as automata can be adopted for aligning traces with this type of models \cite{LeoniMA12,XuLZ17a}.


Multi-perspective checking for process conformance is gaining momentum, as conformance checking techniques considering both trace types and data annotations as ``first-class citizens'' enable to discover more deviations \cite{MultiPerspective}. This reflects the essence of real-world business processes, which are inherently described by both business processes and their different domain objects \cite{PetermannJMR14} (e.g., employees, products, or orders), which can be encoded as traces and event data. While alignment-based  data-aware conformance has been already investigated in the context of procedural models, most of the conformance checking approaches for data-aware declarative models \cite{BurattinMS16,Borrego014} focus on a numerical approximation of the degree of conformance of a log trace against the model and do not provide repair strategies.

%a binary assessment of whether a trace is deviant or not \cite{BurattinMS16} is, to the best of our knowledge, the only work providing a numerical characterization of the process conformance. In this work, the authors reduce the data-aware conformance checking problem to a maximum constraint-satisfaction problem, where both data and model constraints are encoded; even though it provides a numerical approximation of the degree of conformance of a log trace against the model, no repair strategy is provided. On the other hand, the majority of  data-aware conformance checking dealt with procedural models; in this branch, \cite{LeoniA13} combined the A* algorithm for alignment-based control-flow conformance checking in \cite{LeoniMA12} with Integer Linear Programming Problem for data conformance checking. As pointed out in \cite{MultiPerspective}, such solution provides misleading results, as often control-flow and data prospective are closely inter-related in real-world scenarios models \cite{PetermannJMR14}. As a result, a cost function considering both data and control-flow discrepancies is required. Still, it is widely known \cite{AdriansyahDA10} that some trace-alignment strategies do not provide correctness guarantees, as perfectly fitting traces might be assessed as non-fitting executions.

For overcoming this research gap, we propose a novel approach for aligning event logs to data-aware declarative models by reducing it to a data-agnostic alignment problem over LTL$_f$-based models. This solution exploits the following considerations: \begin{enumerate*}[label=\emph{\alph*})]
	\item \label{it1} to represent the process model, we use a sub-set of the data-aware extension of Declare presented in \cite{BurattinMS16}. After representing the data-aware Declare model using a data agnostic LTL$_f$ semantics,
	\item \label{it2} we exploit the data predicates in the data-aware Declare clauses to partition the data space. This provides propositions representing data in addition to event labels. Then,
	\item we combine each event label with the propositions generated in \ref{it2} and transform the model in \ref{it1} into its data-aware counterpart. The automata-based representation of such a model is used to align traces (seen as sequences of events with a payload of data attribute-value pairs) with the model. Specifically, we show that the alignment problem can can be expressed as a planning problem in Artificial Intelligence, which can be efficiently solved by state-of-the-art planners \cite{XuLZ17a,Marrella17}.
%
This will lead to a repair strategy that is able to align traces with a data-aware declarative model based on changes at the level of control flow (such as adding/deleting events) or at the level of the data flow (such as changing the attribute values attached to them).
\end{enumerate*}

The paper is structured as follows: after providing relevant related works (\S\ref{sec:related}), we introduce the notion of event log (\S\ref{ssec:elog}) and the data-aware declarative language used to represent the model (\S\ref{ssec:dad}); we also provide hints on Automated Planning, as we will later exploit the SymBA*-2 optimal planner to compute the alignments (\S\ref{ssec:ap}). These preliminary notions guide us into the definition of our  working assumptions adhering to the literature of reference (\S\ref{sec:wa}). After deep-diving into the technical details providing the solution to the data-aware declarative alignment problem (\S\ref{sec:dccap}), we benchmark SymBA*-2 over a synthetic dataset and discuss its performance in this context (\S\ref{sec:experiments}). Last, we draw our final conclusions and propose some future work (\S\ref{sec:end}).


%we draw the working assumptions jointly with the assumptions from the current literature on declarative conformance checking (\S\ref{sec:wa}). After outlining the declarative model alignment over which we want to reduce the problem (\S\ref{sec:dccap}), we deep-dive into the data-aware Declare Trace Alignment Pipeline (\S\ref{sec:dadtap}) \texttt{\color{red} [TODO: experiments? ]} Last, we draw our final conclusions and propose some future works (\S\texttt{\color{red}[TODO]}). 
