\section{Introduction}
\textit{Conformance checking} is a branch of process mining  assessing whether a sequence of distinguishable events (\textit{trace}) conforms to the expected process behaviors (\textit{model}) \cite{Borrego014}. When such trace does not conform to the model, we say that the trace is \textit{deviant}.  Such conformance is assessed in terms of alignments, i.e., sequence of actions within the trace shall correspond to sequence of actions allowed by the model \cite{LeoniAD12}. When this does not happen,  techniques based on sequence alignment-cost additionally provide minimal repair strategies   to  make the trace conformant to the model \cite{LeoniAD12,XuLZ17a}. This strategy is usually preferred in real-world and dynamic settings such as healthcare \cite{LeoniMA12} and economical transactions \cite{MultiPerspective}, as the combined provision of alternative repair strategies to make a trace conformant ranked by alignment cost empowers an improvement in such scenarios. Models can be described by either procedural or declarative languages: while the former fully enumerates the set of all the possible allowed traces (\textit{unfolding}), the latter provides a compact representation by listing the constraints that specify the expected behaviour \cite{LeoniMA12,XuLZ17a}. Nevertheless, declarative models can be always transformed in procedural ones as constraint automatons  by means of declarative models' formal semantics. In fact, such semantics describes the actions that will follow when some pre-conditions are met \cite{LiPZVR20}. This approach is widely adopted for aligning traces with declarative process models \cite{Westergaard11,LeoniMA12,XuLZ17a}, as traces are mostly described in a procedural fashion.  


Multi-perspective checking for process conformance are gaining momentum, as conformance checking techniques considering both trace types and data annotation as ``first-class citizens'' enable to discover more deviations \cite{MultiPerspective}. This reflects the essence of real-world business processes, which are inherently described by both business processes and their different domain objects \cite{PetermannJMR14} (e.g., employees, products, or orders), that can be encoded as trace and event data. To the best of our knowledge, \cite{Borrego014} is the only work dealing with both aspects in the context of declarative models. The authors reduced the data-aware conformance checking problem to a maximum constraint-satisfaction problem, where both data and model constraints are encoded; even though it provides a numerical approximation of the degree of conformance of a log trace against the model, no repair strategy is provided. On the other hand, the majority of  data-aware conformance checking dealt with procedural models; in this branch, \cite{LeoniA13} combined the A* algorithm for alignment-based control-flow conformance checking in \cite{LeoniMA12} with Integer Linear Programming Problem for data conformance checking. As pointed out in \cite{MultiPerspective}, such solution provides misleading results, as often control-flow and data prospective are closely inter-related in real-world scenarios models \cite{PetermannJMR14}. As a result, a cost function considering both data and control-flow discrepancies is required. Still, it is widely known \cite{AdriansyahDA10} that some trace-alignment strategies do not provide correctness guarantees, as perfectly fitting traces might be assessed as non-fitting executions. 

For overcoming such limitations, we propose a novel approach for aligning event logs to Data-Aware Declare Models by reducing it to a data-agnostic alignment problem over LTL$_f$ based models \cite{XuLZ17a}. This solution exploits the following considerations: \begin{enumerate*}[label=\emph{\alph*})]
	\item after representing a Data-Aware Declare Model over the LTL$_f$ semantics \cite{10.1007/978-3-642-40176-3_8}, 
	\item \label{it2} we can always exploit the data predicates in the declare clauses to partition the data space, which provides additional propositions representing both data and trace-label information;
	\item such partition allows to further decompose  LTL$_f$ propositions. Similarly,
	\item we can replace each trace event with the the propositions generated in \ref{it2}, thus enabling to align such trace to the previous LTL$_f$ model via \cite{XuLZ17a}. Last,
	\item we can ingest the repair strategy generated from \cite{XuLZ17a} to repair both event and data informations.
\end{enumerate*}

The paper is structured as follows: after providing a motivating example (\S\ref{sec:mot}), we draw the working assumptions jointly with the assumptions from current literature on declarative conformance checking (\S\ref{sec:wa}). After outlining the declarative model alignment over which we want to reduce the problem (\S\ref{sec:dccap}), we deep-dive into the Data-Aware Declare Trace Alignment Pipeline (\S\ref{sec:dadtap}) \texttt{\color{red} [TODO: experiments? Also, baseline from BSc Thesis?]} Last, we draw our final conclusions and propose some future works (\S\texttt{\color{red}[TODO]}). 