\section{Introduction}
\textit{Conformance checking} is a branch of process mining  assessing whether a sequence of distinguishable events (a.k.a.\ a \textit{trace}) conforms to the expected process behavior represented as a \textit{process model} \cite{}. When a trace does not conform to the model, we say that the trace is \textit{deviant}. In this case, techniques based on cost-driven alignments additionally provide minimal repair strategies to make the trace conformant to the model \cite{}. Alignments represent a valuable instrument for business analysts, as the combined provision of alternative repair strategies to make a trace conformant ranked by alignment cost supports the business analyst in choosing among different process improvement strategies. In conformance checking, models can be described by either procedural or declarative languages: while the former fully enumerate the set of all the possible allowed traces, the latter provide a compact process representation by listing the constraints that delimit the expected behavior \cite{}. Nevertheless, LTL$_f$-based declarative models can always be transformed into procedural ones by translating their LTL$_f$ semantics into constraint automata.
%In fact, such semantics describes the actions that will follow when some pre-conditions are met \cite{LiPZVR20}.
The representation of declarative models as automata can be adopted for aligning traces with declarative process models \cite{LeoniMA12,XuLZ17a}.


Multi-perspective checking for process conformance is gaining momentum, as conformance checking techniques considering both trace types and data annotations as ``first-class citizens'' enable to discover more deviations \cite{MultiPerspective}. This reflects the essence of real-world business processes, which are inherently described by both business processes and their different domain objects \cite{PetermannJMR14} (e.g., employees, products, or orders), which can be encoded as traces and event data. While alignment-based  data-aware conformance has been already investigated in the context of procedural models, most of the conformance checking approaches for data-aware declarative models \cite{BurattinMS16,Borrego014} focus on a numerical approximation of the degree of conformance of a log trace against the model and no repair strategy is provided.

%a binary assessment of whether a trace is deviant or not \cite{BurattinMS16} is, to the best of our knowledge, the only work providing a numerical characterization of the process conformance. In this work, the authors reduce the data-aware conformance checking problem to a maximum constraint-satisfaction problem, where both data and model constraints are encoded; even though it provides a numerical approximation of the degree of conformance of a log trace against the model, no repair strategy is provided. On the other hand, the majority of  data-aware conformance checking dealt with procedural models; in this branch, \cite{LeoniA13} combined the A* algorithm for alignment-based control-flow conformance checking in \cite{LeoniMA12} with Integer Linear Programming Problem for data conformance checking. As pointed out in \cite{MultiPerspective}, such solution provides misleading results, as often control-flow and data prospective are closely inter-related in real-world scenarios models \cite{PetermannJMR14}. As a result, a cost function considering both data and control-flow discrepancies is required. Still, it is widely known \cite{AdriansyahDA10} that some trace-alignment strategies do not provide correctness guarantees, as perfectly fitting traces might be assessed as non-fitting executions.

For overcoming this research gap, we propose a novel approach for aligning event logs to Data-Aware declarative models by reducing it to a data-agnostic alignment problem over LTL$_f$ based models. This solution exploits the following considerations: \begin{enumerate*}[label=\emph{\alph*})]
	\item \label{it1} to represent the process model, we use a sub-set of Multi-Perspective Declare (MP-Declare). After representing the MP-Declare model using a data agnostic LTL$_f$ semantics \cite{10.1007/978-3-642-40176-3_8},
	\item \label{it2} we exploit the data predicates in the MP-Declare clauses to partition the data space. This provides propositions representing data in addition to event labels. Then,
	\item we can combine each event label with the propositions generated in \ref{it2} and transform the model in \ref{it1} into its data-aware counterpart. The automata-based representation of such a model is used to align traces (seen as sequences of events with a payload of data attribute-value pairs) with the model using the approach presented in \cite{XuLZ17a}. This will lead to a repair strategy that is able to align traces with a data-aware declarative model based on changes at the level of control flow (such as adding/deleting events) or at he level of the data flow (such as changing the attribute values attached to them).
\end{enumerate*}

The paper is structured as follows: after providing a motivating example (\S\ref{sec:mot}), we draw the working assumptions jointly with the assumptions from the current literature on declarative conformance checking (\S\ref{sec:wa}). After outlining the declarative model alignment over which we want to reduce the problem (\S\ref{sec:dccap}), we deep-dive into the Data-Aware Declare Trace Alignment Pipeline (\S\ref{sec:dadtap}) \texttt{\color{red} [TODO: experiments? Also, baseline from BSc Thesis?]} Last, we draw our final conclusions and propose some future works (\S\texttt{\color{red}[TODO]}). 